<!DOCTYPE html>
<html lang ="en">
<head>
    <meta chartset = "UTF-8">
    <<link rel="stylesheet" href="mlData/style.css"/>
    <title>Jack Kingdon</title>
    <script src="p5.js"></script>
</head>
<body>
  <div class="container">
    <div class="top-wrapper">
      <div class="title">
        Devolpment of simple image recognition
        neural network from scratch
      </div>
    </div>
    <div class="middle-wrapper">
      <div class="top-info">
        Since late 2020, with the help of the great Daniel Shiffman aka "the coding train",
        I've been experimenting with how bad you can be at coding and maths to be able
        to build a neural network that could do something interesting*. It turns out pretty bad;
        I managed to make one!
      </div>
      <div class="middle-info">
        Unlike most networks built
        now, I had to do the matrix
        multiplication (sortof) by
        hand (matrixmultiplication.xyz is an amazing website and definately was my best friend during the ordeal) because I wanted to experiance the nitty gritty nature of a network rather than the scary black box that will exterminate all humans in 2024.
      </div>
      <div class="ml-image-wrapper">
        <img class="ml-image" src="img/ml-screencap.png" alt="">
        <div class="image-caption">
          The network doing something?! Getting a 10ish percent
          error rate on MNST data with fairly minimal training.
        </div>
      </div>
    </div>
    <div class="bottom-wrapper">
      <div class="bottom-info">
        This network's original design was heavily "inspired" by Daniel
        Shiffman's toy neural network, although once I got further along I definately was sailing on my own. This is an interesting first ML project considering we live in the age of python libaries with 5 lines of code to produce pure magic to a computer scientist in 1980 (although that is a low bar).
      </div>
    </div>

  </div>
</body>
